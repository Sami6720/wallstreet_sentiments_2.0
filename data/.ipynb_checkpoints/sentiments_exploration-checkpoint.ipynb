{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorations of the data that needs to be collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "import praw\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "# stop warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''includes US stock symbols with market cap > 100 Million, and price above $3. \n",
    "Download the csv file:\n",
    "https://www.nasdaq.com/market-activity/stocks/screener?exchange=nasdaq&letter=0&render=download \n",
    "for all of the NYSE, NASDAQ and NYSEAMERICAN public traded companies.\n",
    "'''\n",
    "\n",
    "stocks = pd.read_csv('tickers.csv')\n",
    "\n",
    "# Last Sale column is an object, let's convert to float\n",
    "stocks['Last Sale'] = stocks['Last Sale'].str.replace('$', '')\n",
    "stocks['Last Sale'] = pd.to_numeric(stocks['Last Sale'], downcast='float')\n",
    "type(stocks['Last Sale'][0])\n",
    "\n",
    "# Filter out stocks >$3 and > $100 million cap\n",
    "price_filter = stocks['Last Sale'] >= 3.00\n",
    "cap_filter = stocks['Market Cap'] >= 100000000\n",
    "\n",
    "# make set of symbols\n",
    "stocks = set(stocks[(price_filter) & (cap_filter)]['Symbol'])\n",
    "\n",
    "# Includes common words and words used on wsb that are also stock names\n",
    "blacklist = {'I', 'ELON', 'WSB', 'THE', 'A', 'ROPE', 'YOLO', 'TOS', 'CEO', 'DD', 'IT', 'OPEN', 'ATH', 'PM', 'IRS', 'FOR','DEC', 'BE', 'IMO', 'ALL', 'RH', 'EV', 'TOS', 'CFO', 'CTO', 'DD', 'BTFD', 'WSB', 'OK', 'PDT', 'RH', 'KYS', 'FD', 'TYS', 'US', 'USA', 'IT', 'ATH', 'RIP', 'BMW', 'GDP', 'OTM', 'ATM', 'ITM', 'IMO', 'LOL', 'AM', 'BE', 'PR', 'PRAY', 'PT', 'FBI', 'SEC', 'GOD', 'NOT', 'POS', 'FOMO', 'TL;DR', 'EDIT', 'STILL', 'WTF', 'RAW', 'PM', 'LMAO', 'LMFAO', 'ROFL', 'EZ', 'RED', 'BEZOS', 'TICK', 'IS', 'PM', 'LPT', 'GOAT', 'FL', 'CA', 'IL', 'MACD', 'HQ', 'OP', 'PS', 'AH', 'TL', 'JAN', 'FEB', 'JUL', 'AUG', 'SEP', 'SEPT', 'OCT', 'NOV', 'FDA', 'IV', 'ER', 'IPO', 'MILF', 'BUT', 'SSN', 'FIFA', 'USD', 'CPU', 'AT', 'GG', 'Mar'}\n",
    "\n",
    "# Adding wsb/reddit flavor to vader to improve sentiment analysis, score: 4.0 to -4.0\n",
    "new_words = {\n",
    "    'citron': -4.0,  \n",
    "    'hidenburg': -4.0,        \n",
    "    'moon': 4.0,\n",
    "    'highs': 2.0,\n",
    "    'mooning': 4.0,\n",
    "    'long': 2.0,\n",
    "    'short': -2.0,\n",
    "    'call': 4.0,\n",
    "    'calls': 4.0,    \n",
    "    'put': -4.0,\n",
    "    'puts': -4.0,    \n",
    "    'break': 2.0,\n",
    "    'tendie': 2.0,\n",
    "     'tendies': 2.0,\n",
    "     'town': 2.0,     \n",
    "     'overvalued': -3.0,\n",
    "     'undervalued': 3.0,\n",
    "     'buy': 4.0,\n",
    "     'sell': -4.0,\n",
    "     'gone': -1.0,\n",
    "     'gtfo': -1.7,\n",
    "     'paper': -1.7,\n",
    "     'bullish': 3.7,\n",
    "     'bearish': -3.7,\n",
    "     'bagholder': -1.7,\n",
    "     'stonk': 1.9,\n",
    "     'green': 1.9,\n",
    "     'money': 1.2,\n",
    "     'print': 2.2,\n",
    "     'rocket': 2.2,\n",
    "     'bull': 2.9,\n",
    "     'bear': -2.9,\n",
    "     'pumping': -1.0,\n",
    "     'sus': -3.0,\n",
    "     'offering': -2.3,\n",
    "     'rip': -4.0,\n",
    "     'downgrade': -3.0,\n",
    "     'upgrade': 3.0,     \n",
    "     'maintain': 1.0,          \n",
    "     'pump': 1.9,\n",
    "     'hot': 1.5,\n",
    "     'drop': -2.5,\n",
    "     'rebound': 1.5,  \n",
    "     'crack': 2.5,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running sentiment analysis, this may take a few minutes...\n",
      "\n",
      "It took 301.30 seconds to analyze 3833 comments in 61 posts in 4 subreddits.\n",
      "\n",
      "Posts analyzed saved in titles\n",
      "\n",
      "10 most mentioned picks: \n",
      "BBBY: 12\n",
      "TSLA: 6\n",
      "NFLX: 3\n",
      "IBKR: 3\n",
      "COST: 3\n",
      "CD: 3\n",
      "ON: 2\n",
      "HAS: 2\n",
      "VERY: 2\n",
      "WBD: 1\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Asus/nltk_data'\n    - 'e:\\\\walstreet_sentiments_two\\\\walstreet_2.0\\\\env\\\\nltk_data'\n    - 'e:\\\\walstreet_sentiments_two\\\\walstreet_2.0\\\\env\\\\share\\\\nltk_data'\n    - 'e:\\\\walstreet_sentiments_two\\\\walstreet_2.0\\\\env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 103\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39m# Applying sentiment analysis\u001b[39;00m\n\u001b[0;32m    101\u001b[0m scores, s \u001b[39m=\u001b[39m {}, {}\n\u001b[1;32m--> 103\u001b[0m vader \u001b[39m=\u001b[39m SentimentIntensityAnalyzer()\n\u001b[0;32m    104\u001b[0m \u001b[39m# adding custom words from data.py\u001b[39;00m\n\u001b[0;32m    105\u001b[0m vader\u001b[39m.\u001b[39mlexicon\u001b[39m.\u001b[39mupdate(new_words)\n",
      "File \u001b[1;32me:\\walstreet_sentiments_two\\walstreet_2.0\\env\\lib\\site-packages\\nltk\\sentiment\\vader.py:340\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.__init__\u001b[1;34m(self, lexicon_file)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    337\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    338\u001b[0m     lexicon_file\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    339\u001b[0m ):\n\u001b[1;32m--> 340\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlexicon_file \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mload(lexicon_file)\n\u001b[0;32m    341\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlexicon \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_lex_dict()\n\u001b[0;32m    342\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconstants \u001b[39m=\u001b[39m VaderConstants()\n",
      "File \u001b[1;32me:\\walstreet_sentiments_two\\walstreet_2.0\\env\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32me:\\walstreet_sentiments_two\\walstreet_2.0\\env\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[1;32me:\\walstreet_sentiments_two\\walstreet_2.0\\env\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Asus/nltk_data'\n    - 'e:\\\\walstreet_sentiments_two\\\\walstreet_2.0\\\\env\\\\nltk_data'\n    - 'e:\\\\walstreet_sentiments_two\\\\walstreet_2.0\\\\env\\\\share\\\\nltk_data'\n    - 'e:\\\\walstreet_sentiments_two\\\\walstreet_2.0\\\\env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRunning sentiment analysis, this may take a few minutes...\\n\")\n",
    "\n",
    "# Instantiate praw object\n",
    "start_time = time.time()\n",
    "reddit = praw.Reddit(\n",
    "    user_agent = \"Comment Extraction\",\n",
    "    client_id = \"QktIf3FZ10C5jg\",\n",
    "    client_secret = \"52XN7-d-s7tBO8Wld9mPQ19PMaV8HA\"\n",
    ")\n",
    "\n",
    "# Set program parameters\n",
    "subs = ['wallstreetbets', 'stocks', 'investing', 'stockmarket']     # sub-reddit to search\n",
    "post_flairs = {'Daily Discussion', 'Weekend Discussion', 'Discussion'}    # posts flairs to search || None flair is automatically considered\n",
    "goodAuth = {'AutoModerator'}   # authors whom comments are allowed more than once\n",
    "uniqueCmt = True                # allow one comment per author per symbol\n",
    "ignoreAuthP = {'example'}       # authors to ignore for posts \n",
    "ignoreAuthC = {'example'}       # authors to ignore for comment \n",
    "upvoteRatio = 0.70         # upvote ratio for post to be considered, 0.70 = 70%\n",
    "ups = 20       # define # of upvotes, post is considered if upvotes exceed this #\n",
    "limit = 10      # define the limit, comments 'replace more' limit\n",
    "upvotes = 2     # define # of upvotes, comment is considered if upvotes exceed this #\n",
    "picks = 10     # define # of picks here, prints as \"Top ## picks are:\"\n",
    "picks_ayz = 10   # define # of picks for sentiment analysis\n",
    "\n",
    "posts, count, c_analyzed, tickers, titles, a_comments = 0, 0, 0, {}, [], {}\n",
    "cmt_auth = {}\n",
    "\n",
    "for sub in subs:\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    hot_python = subreddit.hot() # sorting posts by hot\n",
    "    # Extracting comments, symbols from subreddit\n",
    "    for submission in hot_python:\n",
    "        flair = submission.link_flair_text\n",
    "        author = submission.author.name\n",
    "\n",
    "        # Checking: post upvote ratio # of upvotes, post flair, and author\n",
    "        if submission.upvote_ratio >= upvoteRatio and submission.ups > ups and (flair in post_flairs or flair is None) and author not in ignoreAuthP:\n",
    "            submission.comment_sort = 'new'\n",
    "            comments = submission.comments\n",
    "            titles.append(submission.title)\n",
    "            posts += 1\n",
    "            submission.comments.replace_more(limit = limit)\n",
    "            for comment in comments:\n",
    "                # try except for deleted account?\n",
    "                try:\n",
    "                    auth = comment.author.name\n",
    "                except:\n",
    "                    pass\n",
    "                c_analyzed += 1\n",
    "\n",
    "                # checking: comment upvotes and author\n",
    "                if comment.score > upvotes and auth not in ignoreAuthC:\n",
    "                    split = comment.body.split(' ')\n",
    "                    for word in split:\n",
    "                        word = word.replace(\"$\", \"\")\n",
    "                        # upper = ticker, length of ticker <= 5, excluded words\n",
    "                        if word.isupper() and len(word) <= 5 and word not in blacklist and word in stocks:\n",
    "                            \n",
    "                            # unique comments, try/except for key errors\n",
    "                            if uniqueCmt and auth not in goodAuth:\n",
    "                                try:\n",
    "                                    if auth in cmt_auth[word]:\n",
    "                                        break\n",
    "                                except:\n",
    "                                    pass\n",
    "                            \n",
    "                            # counting tickers\n",
    "                            if word in tickers:\n",
    "                                tickers[word] += 1\n",
    "                                a_comments[word].append(comment.body)\n",
    "                                cmt_auth[word].append(auth)\n",
    "                                count += 1\n",
    "                            else:\n",
    "                                tickers[word] = 1\n",
    "                                cmt_auth[word] = [auth]\n",
    "                                a_comments[word] = [comment.body]\n",
    "                                count += 1\n",
    "\n",
    "# sorts the dictionary\n",
    "symbols = dict(sorted(tickers.items(), key=lambda item: item[1], reverse = True))\n",
    "top_picks = list(symbols.keys())[0:picks]\n",
    "time = (time.time() - start_time)\n",
    "\n",
    "# print top picks\n",
    "print(\"It took {t:.2f} seconds to analyze {c} comments in {p} posts in {s} subreddits.\\n\".format(t=time,\n",
    "                                                                                                c=c_analyzed,\n",
    "                                                                                                p=posts,\n",
    "                                                                                                s=len(subs)))\n",
    "print(\"Posts analyzed saved in titles\")\n",
    "# for i in titles: print(i) # prints the title of the posts analyzed\n",
    "\n",
    "print(f\"\\n{picks} most mentioned picks: \")\n",
    "times = []\n",
    "top = []\n",
    "for i in top_picks:\n",
    "    print(f\"{i}: {symbols[i]}\")\n",
    "    times.append(symbols[i])\n",
    "    top.append(f\"{i}: {symbols[i]}\")\n",
    "\n",
    "# Applying sentiment analysis\n",
    "scores, s = {}, {}\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "# adding custom words from data.py\n",
    "vader.lexicon.update(new_words)\n",
    "\n",
    "picks_sentiment = list(symbols.keys())[0: picks_ayz]\n",
    "for symbol in picks_sentiment:\n",
    "    stock_comments = a_comments[symbol]\n",
    "    for cmnt in stock_comments:\n",
    "        score = vader.polarity_scores(cmnt)\n",
    "        if symbol in s:\n",
    "            s[symbol][cmnt] = score\n",
    "        else:\n",
    "            s[symbol] = {cmnt: score}\n",
    "        if symbol in scores:\n",
    "            for key, _ in score.items():\n",
    "                scores[symbol][key] += score[key]\n",
    "        else:\n",
    "            scores[symbol] = score\n",
    "\n",
    "    # calculating averages\n",
    "    for key in score:\n",
    "        scores[symbol][key] = scores[symbol][key] / symbols[symbol]\n",
    "        scores[symbol][key] = \"{pol:.3f}\".format(pol=scores[symbol][key])\n",
    "\n",
    "# Printing sentiment analysis\n",
    "print(f\"\\nSentiment analysis of top {picks_ayz} picks:\")\n",
    "df = pd.DataFrame(scores)\n",
    "df.index = ['Bearish', 'Neutral', 'Bullish', 'Total_Compound']\n",
    "df = df.T\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "85676b5b98064dc67740f0fe05b014afeeee6e0f81ace4500c826ea813768386"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
