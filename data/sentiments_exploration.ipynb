{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorations of the data that needs to be collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "import praw\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from datetime import date\n",
    "\n",
    "# stop warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''includes US stock symbols with market cap > 100 Million, and price above $3. \n",
    "Download the csv file:\n",
    "https://www.nasdaq.com/market-activity/stocks/screener?exchange=nasdaq&letter=0&render=download \n",
    "for all of the NYSE, NASDAQ and NYSEAMERICAN public traded companies.\n",
    "'''\n",
    "\n",
    "stocks = pd.read_csv('tickers.csv')\n",
    "\n",
    "# Last Sale column is an object, let's convert to float\n",
    "stocks['Last Sale'] = stocks['Last Sale'].str.replace('$', '')\n",
    "stocks['Last Sale'] = pd.to_numeric(stocks['Last Sale'], downcast='float')\n",
    "type(stocks['Last Sale'][0])\n",
    "\n",
    "# Filter out stocks >$3 and > $100 million cap\n",
    "price_filter = stocks['Last Sale'] >= 3.00\n",
    "cap_filter = stocks['Market Cap'] >= 100000000\n",
    "\n",
    "# make set of symbols\n",
    "stocks = set(stocks[(price_filter) & (cap_filter)]['Symbol'])\n",
    "\n",
    "# Includes common words and words used on wsb that are also stock names\n",
    "blacklist = {'I', 'ELON', 'WSB', 'THE', 'A', 'ROPE', 'YOLO', 'TOS', 'CEO', 'DD', 'IT', 'OPEN', 'ATH', 'PM', 'IRS', 'FOR','DEC', 'BE', 'IMO', 'ALL', 'RH', 'EV', 'TOS', 'CFO', 'CTO', 'DD', 'BTFD', 'WSB', 'OK', 'PDT', 'RH', 'KYS', 'FD', 'TYS', 'US', 'USA', 'IT', 'ATH', 'RIP', 'BMW', 'GDP', 'OTM', 'ATM', 'ITM', 'IMO', 'LOL', 'AM', 'BE', 'PR', 'PRAY', 'PT', 'FBI', 'SEC', 'GOD', 'NOT', 'POS', 'FOMO', 'TL;DR', 'EDIT', 'STILL', 'WTF', 'RAW', 'PM', 'LMAO', 'LMFAO', 'ROFL', 'EZ', 'RED', 'BEZOS', 'TICK', 'IS', 'PM', 'LPT', 'GOAT', 'FL', 'CA', 'IL', 'MACD', 'HQ', 'OP', 'PS', 'AH', 'TL', 'JAN', 'FEB', 'JUL', 'AUG', 'SEP', 'SEPT', 'OCT', 'NOV', 'FDA', 'IV', 'ER', 'IPO', 'MILF', 'BUT', 'SSN', 'FIFA', 'USD', 'CPU', 'AT', 'GG', 'Mar'}\n",
    "\n",
    "# Adding wsb/reddit flavor to vader to improve sentiment analysis, score: 4.0 to -4.0\n",
    "new_words = {\n",
    "    'citron': -4.0,  \n",
    "    'hidenburg': -4.0,        \n",
    "    'moon': 4.0,\n",
    "    'highs': 2.0,\n",
    "    'mooning': 4.0,\n",
    "    'long': 2.0,\n",
    "    'short': -2.0,\n",
    "    'call': 4.0,\n",
    "    'calls': 4.0,    \n",
    "    'put': -4.0,\n",
    "    'puts': -4.0,    \n",
    "    'break': 2.0,\n",
    "    'tendie': 2.0,\n",
    "     'tendies': 2.0,\n",
    "     'town': 2.0,     \n",
    "     'overvalued': -3.0,\n",
    "     'undervalued': 3.0,\n",
    "     'buy': 4.0,\n",
    "     'sell': -4.0,\n",
    "     'gone': -1.0,\n",
    "     'gtfo': -1.7,\n",
    "     'paper': -1.7,\n",
    "     'bullish': 3.7,\n",
    "     'bearish': -3.7,\n",
    "     'bagholder': -1.7,\n",
    "     'stonk': 1.9,\n",
    "     'green': 1.9,\n",
    "     'money': 1.2,\n",
    "     'print': 2.2,\n",
    "     'rocket': 2.2,\n",
    "     'bull': 2.9,\n",
    "     'bear': -2.9,\n",
    "     'pumping': -1.0,\n",
    "     'sus': -3.0,\n",
    "     'offering': -2.3,\n",
    "     'rip': -4.0,\n",
    "     'downgrade': -3.0,\n",
    "     'upgrade': 3.0,     \n",
    "     'maintain': 1.0,          \n",
    "     'pump': 1.9,\n",
    "     'hot': 1.5,\n",
    "     'drop': -2.5,\n",
    "     'rebound': 1.5,  \n",
    "     'crack': 2.5,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running sentiment analysis, this may take a few minutes...\n",
      "\n",
      "It took 279.25 seconds to analyze 3599 comments in 59 posts in 4 subreddits.\n",
      "\n",
      "Posts analyzed saved in titles\n",
      "\n",
      "10 most mentioned picks: \n",
      "BBBY: 15\n",
      "TSLA: 4\n",
      "NFLX: 4\n",
      "IBKR: 3\n",
      "COST: 3\n",
      "CD: 3\n",
      "AAPL: 2\n",
      "HAS: 2\n",
      "MSFT: 2\n",
      "VERY: 2\n",
      "\n",
      "Sentiment analysis of top 10 picks:\n",
      "     Bearish Neutral Bullish Total_Compound\n",
      "BBBY   0.056   0.833   0.112          0.158\n",
      "TSLA   0.168   0.689   0.143         -0.188\n",
      "NFLX   0.135   0.684   0.180          0.196\n",
      "IBKR   0.013   0.786   0.200          0.638\n",
      "COST   0.022   0.894   0.084          0.352\n",
      "CD     0.066   0.830   0.103          0.311\n",
      "AAPL   0.012   0.843   0.144          0.276\n",
      "HAS    0.128   0.549   0.323         -0.077\n",
      "MSFT   0.095   0.753   0.152          0.192\n",
      "VERY   0.096   0.778   0.127          0.361\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRunning sentiment analysis, this may take a few minutes...\\n\")\n",
    "\n",
    "# Instantiate praw object\n",
    "start_time = time.time()\n",
    "reddit = praw.Reddit(\n",
    "    user_agent = \"Comment Extraction\",\n",
    "    client_id = \"QktIf3FZ10C5jg\",\n",
    "    client_secret = \"52XN7-d-s7tBO8Wld9mPQ19PMaV8HA\"\n",
    ")\n",
    "\n",
    "# Set program parameters\n",
    "subs = ['wallstreetbets', 'stocks', 'investing', 'stockmarket']     # sub-reddit to search\n",
    "post_flairs = {'Daily Discussion', 'Weekend Discussion', 'Discussion'}    # posts flairs to search || None flair is automatically considered\n",
    "goodAuth = {'AutoModerator'}   # authors whom comments are allowed more than once\n",
    "uniqueCmt = True                # allow one comment per author per symbol\n",
    "ignoreAuthP = {'example'}       # authors to ignore for posts \n",
    "ignoreAuthC = {'example'}       # authors to ignore for comment \n",
    "upvoteRatio = 0.70         # upvote ratio for post to be considered, 0.70 = 70%\n",
    "ups = 20       # define # of upvotes, post is considered if upvotes exceed this #\n",
    "limit = 10      # define the limit, comments 'replace more' limit\n",
    "upvotes = 2     # define # of upvotes, comment is considered if upvotes exceed this #\n",
    "picks = 10     # define # of picks here, prints as \"Top ## picks are:\"\n",
    "picks_ayz = 10   # define # of picks for sentiment analysis\n",
    "\n",
    "posts, count, c_analyzed, tickers, titles, a_comments = 0, 0, 0, {}, [], {}\n",
    "cmt_auth = {}\n",
    "\n",
    "for sub in subs:\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    hot_python = subreddit.hot() # sorting posts by hot\n",
    "    # Extracting comments, symbols from subreddit\n",
    "    for submission in hot_python:\n",
    "        flair = submission.link_flair_text\n",
    "        author = submission.author.name\n",
    "\n",
    "        # Checking: post upvote ratio # of upvotes, post flair, and author\n",
    "        if submission.upvote_ratio >= upvoteRatio and submission.ups > ups and (flair in post_flairs or flair is None) and author not in ignoreAuthP:\n",
    "            submission.comment_sort = 'new'\n",
    "            comments = submission.comments\n",
    "            titles.append(submission.title)\n",
    "            posts += 1\n",
    "            submission.comments.replace_more(limit = limit)\n",
    "            for comment in comments:\n",
    "                # try except for deleted account?\n",
    "                try:\n",
    "                    auth = comment.author.name\n",
    "                except:\n",
    "                    pass\n",
    "                c_analyzed += 1\n",
    "\n",
    "                # checking: comment upvotes and author\n",
    "                if comment.score > upvotes and auth not in ignoreAuthC:\n",
    "                    split = comment.body.split(' ')\n",
    "                    for word in split:\n",
    "                        word = word.replace(\"$\", \"\")\n",
    "                        # upper = ticker, length of ticker <= 5, excluded words\n",
    "                        if word.isupper() and len(word) <= 5 and word not in blacklist and word in stocks:\n",
    "                            \n",
    "                            # unique comments, try/except for key errors\n",
    "                            if uniqueCmt and auth not in goodAuth:\n",
    "                                try:\n",
    "                                    if auth in cmt_auth[word]:\n",
    "                                        break\n",
    "                                except:\n",
    "                                    pass\n",
    "                            \n",
    "                            # counting tickers\n",
    "                            if word in tickers:\n",
    "                                tickers[word] += 1\n",
    "                                a_comments[word].append(comment.body)\n",
    "                                cmt_auth[word].append(auth)\n",
    "                                count += 1\n",
    "                            else:\n",
    "                                tickers[word] = 1\n",
    "                                cmt_auth[word] = [auth]\n",
    "                                a_comments[word] = [comment.body]\n",
    "                                count += 1\n",
    "\n",
    "# sorts the dictionary\n",
    "symbols = dict(sorted(tickers.items(), key=lambda item: item[1], reverse = True))\n",
    "top_picks = list(symbols.keys())[0:picks]\n",
    "time = (time.time() - start_time)\n",
    "\n",
    "# print top picks\n",
    "print(\"It took {t:.2f} seconds to analyze {c} comments in {p} posts in {s} subreddits.\\n\".format(t=time,\n",
    "                                                                                                c=c_analyzed,\n",
    "                                                                                                p=posts,\n",
    "                                                                                                s=len(subs)))\n",
    "print(\"Posts analyzed saved in titles\")\n",
    "# for i in titles: print(i) # prints the title of the posts analyzed\n",
    "\n",
    "print(f\"\\n{picks} most mentioned picks: \")\n",
    "times = []\n",
    "top = []\n",
    "for i in top_picks:\n",
    "    print(f\"{i}: {symbols[i]}\")\n",
    "    times.append(symbols[i])\n",
    "    top.append(f\"{i}: {symbols[i]}\")\n",
    "\n",
    "# Applying sentiment analysis\n",
    "scores, s = {}, {}\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "# adding custom words from data.py\n",
    "vader.lexicon.update(new_words)\n",
    "\n",
    "picks_sentiment = list(symbols.keys())[0: picks_ayz]\n",
    "for symbol in picks_sentiment:\n",
    "    stock_comments = a_comments[symbol]\n",
    "    for cmnt in stock_comments:\n",
    "        score = vader.polarity_scores(cmnt)\n",
    "        if symbol in s:\n",
    "            s[symbol][cmnt] = score\n",
    "        else:\n",
    "            s[symbol] = {cmnt: score}\n",
    "        if symbol in scores:\n",
    "            for key, _ in score.items():\n",
    "                scores[symbol][key] += score[key]\n",
    "        else:\n",
    "            scores[symbol] = score\n",
    "\n",
    "    # calculating averages\n",
    "    for key in score:\n",
    "        scores[symbol][key] = scores[symbol][key] / symbols[symbol]\n",
    "        scores[symbol][key] = \"{pol:.3f}\".format(pol=scores[symbol][key])\n",
    "\n",
    "# Printing sentiment analysis\n",
    "print(f\"\\nSentiment analysis of top {picks_ayz} picks:\")\n",
    "df = pd.DataFrame(scores)\n",
    "df.index = ['Bearish', 'Neutral', 'Bullish', 'Total_Compound']\n",
    "df = df.T\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "85676b5b98064dc67740f0fe05b014afeeee6e0f81ace4500c826ea813768386"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
